workDir = "$MYSCRATCH/vcspipe_work"

params {
    cluster = 'setonix'

    // Accounts and partitions
    cpu_acct = 'pawsey1154'
    cpu_queue = 'work'
    gpu_acct = 'pawsey1154-gpu'
    gpu_queue = 'gpu'

    // Paths
    vcs_dir = "$MYSCRATCH/vcs_downloads"
    pyvenv  = "$MYSOFTWARE/setonix/python/venvs/vcspipe"
    tools_cont = '/software/projects/mwavcs/cplee/containers/vcspipe-tools.sif'

    // Versions
    python_ver       = '3.11.6'
    singularity_ver  = '4.1.0-mwavcs'
    giant_squid_ver  = '2.2.0'
    vcsbeam_ver      = 'v5.4'
    psr_analysis_ver = '24-12-11'
    psr_search_ver   = '25-08-14'

    // Options
    asvo_api_key = "$MWA_ASVO_API_KEY"
    mwa_beam_file = "$MWA_BEAM_FILE"
}

executor {
    $slurm {
        // Number of tasks handled in parallel
        queueSize = 30
        // Job submission rate
        submitRateLimit = '20/1min'
        // How often to poll the job status
        pollInterval = 30.s
        // Naming scheme
        jobName = { "${task.process}_(${task.index})" }
    }
    $local {
        // Local tasks should not be resource intensive
        cpus   = 1
        memory = 4.GB
    }
}

process {
    cache = 'lenient'

    withLabel: cluster {
        executor = 'slurm'
    }

    withLabel: python {
        beforeScript = "module load python/${params.python_ver}; . ${params.pyvenv}/bin/activate"
    }

    withLabel: giantsquid {
        beforeScript = "export MWA_ASVO_API_KEY='${params.asvo_api_key}'; module load giant-squid/${params.giant_squid_ver}"
    }

    // Local processes
    withName: GET_OBS_METADATA {
        beforeScript = "module load psr-analysis/${params.psr_analysis_ver}; module load python/${params.python_ver}; . ${params.pyvenv}/bin/activate"
    }

    withName: GET_EPHEMERIS {
        beforeScript = "module load psr-analysis/${params.psr_analysis_ver}"
    }

    // GPU node processes
    withName: VCSBEAM {
        time = { Integer.valueOf(params.vcsbeam_wt) * 1.h }
        maxForks = Integer.valueOf(params.vcsbeam_forks)
        clusterOptions = { "-A ${params.gpu_acct} -p ${params.gpu_queue} -N ${Math.ceil(Float.valueOf(params.num_chan) / 8.0) as int} -n ${params.num_chan} --gpus-per-task=1" }
        beforeScript = "module load vcsbeam/${params.vcsbeam_ver}"
    }

    // CPU node processes
    // Note that Pawsey recommends using a multiple of 8 cores as this is the number of cores
    // per chiplet of the AMP processors. This will ensure the optimal L3 cache utilisation.
    withName: TAR {
        time = { Integer.valueOf(params.tar_wt) * 1.h }
        clusterOptions = { "-A ${params.cpu_acct} -p ${params.cpu_queue} -N 1 -n 4 -c 1 --mem=32G" }
    }

    withName: UNTAR {
        time = { Integer.valueOf(params.untar_wt) * 1.h }
        clusterOptions = { "-A ${params.cpu_acct} -p ${params.cpu_queue} -N 1 -n 1 -c 1 --mem=7G" }
    }

    withName: DSPSR {
        time = { Integer.valueOf(params.dspsr_wt) * 1.h }
        clusterOptions = { "-A ${params.cpu_acct} -p ${params.cpu_queue} -N 1 -n 1 -c 8 --mem=115G" }
        beforeScript = "module load psr-analysis/${params.psr_analysis_ver}"
    }

    withName: CLFD {
        time = { Integer.valueOf(params.clfd_wt) * 1.h }
        clusterOptions = { "-A ${params.cpu_acct} -p ${params.cpu_queue} -N 1 -n 1 -c 8 --mem=28G" }
        beforeScript = "module load psr-analysis/${params.psr_analysis_ver}"
    }

    withName: PAV {
        time = { Integer.valueOf(params.pav_wt) * 1.h }
        clusterOptions = { "-A ${params.cpu_acct} -p ${params.cpu_queue} -N 1 -n 1 -c 1 --mem=14G" }
        beforeScript = "module load psr-analysis/${params.psr_analysis_ver}"
    }

    withName: PDMP {
        time = { Integer.valueOf(params.pdmp_wt) * 1.h }
        clusterOptions = { "-A ${params.cpu_acct} -p ${params.cpu_queue} -N 1 -n 1 -c 1 --mem=14G" }
        beforeScript = "module load psr-analysis/${params.psr_analysis_ver}; module load python/${params.python_ver}"
    }

    withName: PREPFOLD {
        time = { Integer.valueOf(params.prepfold_wt) * 1.h }
        clusterOptions = { "-A ${params.cpu_acct} -p ${params.cpu_queue} -N 1 -n 1 -c 4 --mem=7G" }
        beforeScript = "module load psr-search/${params.psr_search_ver}"
    }

    withName: FLUXCAL {
        time = { Integer.valueOf(params.fluxcal_wt) * 1.h }
        clusterOptions = { "-A ${params.cpu_acct} -p ${params.cpu_queue} -N 1 -n 1 -c 16 --mem=28G" }
        beforeScript = "export MWA_BEAM_FILE='${params.mwa_beam_file}'; module load singularity/${params.singularity_ver}"
    }

    withName: RMSYNTH {
        time = { Integer.valueOf(params.rmsynth_wt) * 1.h }
        clusterOptions = { "-A ${params.cpu_acct} -p ${params.cpu_queue} -N 1 -n 1 -c 32 --mem=57G" }
        beforeScript = "module load singularity/${params.singularity_ver}"
    }

    withName: CALIBRATE_ARCHIVE {
        time = { Integer.valueOf(params.cal_wt) * 1.h }
        clusterOptions = { "-A ${params.cpu_acct} -p ${params.cpu_queue} -N 1 -n 1 -c 1 --mem=14G" }
        beforeScript = "module load psr-analysis/${params.psr_analysis_ver}; module load python/${params.python_ver}; . ${params.pyvenv}/bin/activate"
    }

    withName: TRUNCATE_ARCHIVE {
        time = { 1.h }
        clusterOptions = { "-A ${params.cpu_acct} -p ${params.cpu_queue} -N 1 -n 1 -c 1 --mem=28G" }
        beforeScript = "export MWA_BEAM_FILE='${params.mwa_beam_file}'; module load singularity/${params.singularity_ver}; module load psr-analysis/${params.psr_analysis_ver}"
    }
}
